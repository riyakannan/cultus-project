# ================================
# 1. IMPORT LIBRARIES
# ================================
import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Layer, Dropout
from tensorflow.keras.optimizers import Adam

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# ================================
# 2. DATA GENERATION (5 FEATURES)
# ================================
np.random.seed(42)

time_steps = 1200
t = np.arange(time_steps)

f1 = np.sin(0.02 * t) + np.random.normal(0, 0.1, time_steps)
f2 = np.cos(0.02 * t) + np.random.normal(0, 0.1, time_steps)
f3 = 0.5 * f1 + np.random.normal(0, 0.05, time_steps)
f4 = 0.3 * f2 + np.random.normal(0, 0.05, time_steps)
f5 = f1 + f2 + np.random.normal(0, 0.1, time_steps)

data = np.vstack([f1, f2, f3, f4, f5]).T
df = pd.DataFrame(data, columns=[f'feature_{i+1}' for i in range(5)])

# ================================
# 3. NORMALIZATION
# ================================
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df.values)

# ================================
# 4. SEQUENCE CREATION
# ================================
def create_sequences(data, input_len=30, output_len=1):
    X, y = [], []
    for i in range(len(data) - input_len - output_len):
        X.append(data[i:i+input_len])
        y.append(data[i+input_len:i+input_len+output_len])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_data)

# ================================
# 5. TRAIN / VAL / TEST SPLIT
# ================================
train_size = int(0.7 * len(X))
val_size = int(0.15 * len(X))

X_train, y_train = X[:train_size], y[:train_size]
X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]

# ================================
# 6. CUSTOM SELF-ATTENTION LAYER
# ================================
class SelfAttention(Layer):
    def __init__(self, units):
        super(SelfAttention, self).__init__()
        self.Wq = Dense(units)
        self.Wk = Dense(units)
        self.Wv = Dense(units)

    def call(self, inputs):
        Q = self.Wq(inputs)
        K = self.Wk(inputs)
        V = self.Wv(inputs)

        scores = tf.matmul(Q, K, transpose_b=True)
        dk = tf.cast(tf.shape(K)[-1], tf.float32)
        scaled_scores = scores / tf.math.sqrt(dk)

        weights = tf.nn.softmax(scaled_scores, axis=-1)
        output = tf.matmul(weights, V)
        return output

# ================================
# 7. ATTENTION-BASED MODEL
# ================================
inputs = Input(shape=(X.shape[1], X.shape[2]))

x = LSTM(64, return_sequences=True)(inputs)
x = SelfAttention(64)(x)
x = LSTM(32)(x)
x = Dropout(0.2)(x)
outputs = Dense(5)(x)

attention_model = Model(inputs, outputs)
attention_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse'
)

print("\nAttention Model Summary")
attention_model.summary()

# ================================
# 8. TRAIN ATTENTION MODEL
# ================================
attention_model.fit(
    X_train, y_train.squeeze(),
    validation_data=(X_val, y_val.squeeze()),
    epochs=20,
    batch_size=32
)

# ================================
# 9. BASELINE LSTM MODEL
# ================================
baseline_inputs = Input(shape=(X.shape[1], X.shape[2]))
x = LSTM(64)(baseline_inputs)
baseline_outputs = Dense(5)(x)

baseline_model = Model(baseline_inputs, baseline_outputs)
baseline_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse'
)

print("\nBaseline Model Summary")
baseline_model.summary()

baseline_model.fit(
    X_train, y_train.squeeze(),
    validation_data=(X_val, y_val.squeeze()),
    epochs=20,
    batch_size=32
)

# ================================
# 10. EVALUATION
# ================================
att_pred = attention_model.predict(X_test)
base_pred = baseline_model.predict(X_test)

y_true = y_test.squeeze()

def evaluate(y_true, y_pred, name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    print(f"{name} -> MAE: {mae:.4f}, RMSE: {rmse:.4f}")

print("\nModel Performance")
evaluate(y_true, att_pred, "Attention Model")
evaluate(y_true, base_pred, "Baseline LSTM")

# ================================
# 11. FINAL PREDICTION (DELIVERABLE)
# ================================
final_prediction = attention_model.predict(X_test[:1])
final_prediction = scaler.inverse_transform(final_prediction)

print("\nFinal Forecast (Next Time Step):")
print(final_prediction)
