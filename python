# ============================================
# 1. IMPORT LIBRARIES
# ============================================
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Model
from sklearn.preprocessing import MinMaxScaler

# ============================================
# 2. GENERATE SYNTHETIC HIGH-DIMENSIONAL DATA
# ============================================
np.random.seed(42)

time_steps = 1500
num_features = 12
t = np.arange(time_steps)

data = []
for i in range(num_features):
    signal = np.sin(0.01 * t + i) + np.random.normal(0, 0.05, time_steps)
    data.append(signal)

data = np.array(data).T

# Inject anomalies
anomaly_points = np.random.choice(range(300, 1200), size=40, replace=False)
data[anomaly_points] += np.random.normal(3, 0.5, (40, num_features))

# ============================================
# 3. NORMALIZE DATA
# ============================================
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data)

# ============================================
# 4. CREATE TIME-WINDOW SEQUENCES
# ============================================
def create_sequences(data, window_size=20):
    sequences = []
    for i in range(len(data) - window_size):
        sequences.append(data[i:i + window_size])
    return np.array(sequences)

window_size = 20
X = create_sequences(data_scaled, window_size)

# ============================================
# 5. TRAIN ONLY ON NORMAL DATA
# ============================================
normal_X = X[:900]
test_X = X

# ============================================
# 6. REPARAMETERIZATION (SAMPLING) LAYER
# ============================================
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        epsilon = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

# ============================================
# 7. BUILD ENCODER
# ============================================
latent_dim = 5

encoder_inputs = layers.Input(shape=(window_size, num_features))
x = layers.Flatten()(encoder_inputs)
x = layers.Dense(64, activation="relu")(x)
x = layers.Dense(32, activation="relu")(x)

z_mean = layers.Dense(latent_dim)(x)
z_log_var = layers.Dense(latent_dim)(x)
z = Sampling()([z_mean, z_log_var])

encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder")

# ============================================
# 8. BUILD DECODER
# ============================================
latent_inputs = layers.Input(shape=(latent_dim,))
x = layers.Dense(32, activation="relu")(latent_inputs)
x = layers.Dense(64, activation="relu")(x)
x = layers.Dense(window_size * num_features, activation="sigmoid")(x)
decoder_outputs = layers.Reshape((window_size, num_features))(x)

decoder = Model(latent_inputs, decoder_outputs, name="decoder")

# ============================================
# 9. BUILD VAE WITH CUSTOM ELBO LOSS
# ============================================
class VAE(Model):
    def __init__(self, encoder, decoder, beta=1.0):
        super(VAE, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.beta = beta

    def train_step(self, data):
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)

            reconstruction_loss = tf.reduce_mean(
                tf.reduce_sum(tf.square(data - reconstruction), axis=[1, 2])
            )

            kl_loss = -0.5 * tf.reduce_mean(
                tf.reduce_sum(
                    1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var),
                    axis=1,
                )
            )

            total_loss = reconstruction_loss + self.beta * kl_loss

        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        return {
            "loss": total_loss,
            "reconstruction_loss": reconstruction_loss,
            "kl_loss": kl_loss,
        }

# ============================================
# 10. COMPILE AND TRAIN VAE
# ============================================
vae = VAE(encoder, decoder, beta=1.0)
vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))

vae.fit(
    normal_X,
    epochs=30,
    batch_size=32,
    shuffle=True
)

# ============================================
# 11. RECONSTRUCTION ERROR COMPUTATION
# ============================================
z_mean, z_log_var, z = encoder.predict(test_X)
reconstructions = decoder.predict(z)

reconstruction_errors = np.mean(
    np.square(test_X - reconstructions), axis=(1, 2)
)

# ============================================
# 12. ANOMALY THRESHOLD (STATISTICAL)
# ============================================
threshold = np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors)

# ============================================
# 13. TOP 5 ANOMALOUS TIME WINDOWS
# ============================================
top_anomalies = np.argsort(reconstruction_errors)[-5:][::-1]

print("\nTop 5 Anomalous Time Windows:")
for idx in top_anomalies:
    print(f"Index: {idx}, Reconstruction Error: {reconstruction_errors[idx]:.4f}")

print("\nAnomaly Threshold:", threshold)
